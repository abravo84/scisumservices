<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">09-PONE-RA-08552R1</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0005372</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computer Science</subject><subject>Mathematics/Statistics</subject><subject>Physics/Interdisciplinary Physics</subject></subj-group></article-categories><title-group><article-title>Modeling Statistical Properties of Written Text</article-title><alt-title alt-title-type="running-head">Modeling Written Text</alt-title></title-group><contrib-group>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Serrano</surname><given-names>M. Ángeles</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Flammini</surname><given-names>Alessandro</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Menczer</surname><given-names>Filippo</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
</contrib-group><aff id="aff1"><label>1</label><addr-line>Departament de Química Física, Universitat de Barcelona, Barcelona, Spain</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>School of Informatics, Indiana University, Bloomington, Indiana, United States of America</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>Complex Networks Lagrange Lab, ISI Foundation, Torino, Italy</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Scalas</surname><given-names>Enrico</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">University of East Piedmont, Italy</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">marian.serrano@ub.edu</email></corresp>
<fn fn-type="con"><p>Designed research, performed research, analyzed data, and wrote the paper: MAS AF FM.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><year>2009</year></pub-date><pub-date pub-type="epub"><day>29</day><month>4</month><year>2009</year></pub-date><volume>4</volume><issue>4</issue><elocation-id>e5372</elocation-id><history>
<date date-type="received"><day>5</day><month>2</month><year>2009</year></date>
<date date-type="accepted"><day>31</day><month>3</month><year>2009</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2009</copyright-year><copyright-holder>Serrano et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>Written text is one of the fundamental manifestations of human language, and the study of its universal regularities can give clues about how our brains process information and how we, as a society, organize and share it. Among these regularities, only Zipf's law has been explored in depth. Other basic properties, such as the existence of bursts of rare words in specific documents, have only been studied independently of each other and mainly by descriptive models. As a consequence, there is a lack of understanding of linguistic processes as complex emergent phenomena. Beyond Zipf's law for word frequencies, here we focus on burstiness, Heaps' law describing the sublinear growth of vocabulary size with the length of a document, and the topicality of document collections, which encode correlations within and across documents absent in random null models. We introduce and validate a generative model that explains the simultaneous emergence of all these patterns from simple rules. As a result, we find a connection between the bursty nature of rare words and the topical organization of texts and identify dynamic word ranking and memory across documents as key mechanisms explaining the non trivial organization of written text. Our research can have broad implications and practical applications in computer science, cognitive science and linguistics.</p>
</abstract><funding-group><funding-statement>This work was supported in part by a Lagrange Senior Fellowship of the CRT Foundation to F.M., by the Institute for Scientific Interchange Foundation, the EPFL Laboratory of Statistical Biophysics, and the Indiana University School of Informatics; M. A. S. acknowledges support by DGES grant No. FIS2007-66485-C02-01 and the Ramón y Cajal program of the Spanish Ministry of Science. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="8"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>The understanding of human language <xref ref-type="bibr" rid="pone.0005372-Hauser1">[1]</xref> requires an interdisciplinary approach and has broad conceptual and practical implications over a broad range of fields. Computer science, where natural language processing <xref ref-type="bibr" rid="pone.0005372-Joshi1">[2]</xref>–<xref ref-type="bibr" rid="pone.0005372-Nowak1">[4]</xref> seeks to model language computationally, and cognitive science, that tries to understand our intelligence with linguistics as one of its key contributing disciplines <xref ref-type="bibr" rid="pone.0005372-Chomsky1">[5]</xref>, are among the fields more directly involved.</p>
<p>Written text is a fundamental manifestation of human language. Nowadays, electronic and information technology media offer the opportunity to easily record and access huge amounts of documents that can be analyzed in quest for some of the signatures of human communication. As a first step, statistical patterns in written text can be detected as a trace of the mental processes we use in communication. It has been realized that various universal regularities characterize text from different domains and languages. The best-known is Zipf's law on the distribution of word frequencies <xref ref-type="bibr" rid="pone.0005372-Zipf1">[6]</xref>–<xref ref-type="bibr" rid="pone.0005372-Saichev1">[8]</xref>, according to which the frequency of terms in a collection decreases inversely to the rank of the terms. Zipf's law has been found to apply to collections of written documents in virtually all languages. Other notable universal regularities of text are Heaps' law <xref ref-type="bibr" rid="pone.0005372-Heaps1">[9]</xref>, <xref ref-type="bibr" rid="pone.0005372-Cattuto1">[10]</xref>, according to which vocabulary size grows slowly with document size, i.e. as a sublinear function of the number of words; and the bursty nature of words <xref ref-type="bibr" rid="pone.0005372-Church1">[11]</xref>–<xref ref-type="bibr" rid="pone.0005372-Kleinberg1">[13]</xref>, making a word more likely to reappear in a document if it has already appeared, compared to its overall frequency across the collection.</p>
<p>The structure of written text is key to a broad range of critical applications such as Web search <xref ref-type="bibr" rid="pone.0005372-Chakrabarti1">[14]</xref>, <xref ref-type="bibr" rid="pone.0005372-Liu1">[15]</xref> (and the booming business of online advertising), literature mining <xref ref-type="bibr" rid="pone.0005372-Ananiadou1">[16]</xref>, <xref ref-type="bibr" rid="pone.0005372-Feldman1">[17]</xref>, topic detection <xref ref-type="bibr" rid="pone.0005372-Allan1">[18]</xref>, <xref ref-type="bibr" rid="pone.0005372-Yang1">[19]</xref>, and security <xref ref-type="bibr" rid="pone.0005372-Chen1">[20]</xref>–<xref ref-type="bibr" rid="pone.0005372-Pennebaker1">[22]</xref>. Thus, it is not surprising that researchers in linguistics, information and cognitive science, machine learning, and complex systems are coming together to model how universal text properties emerge. Different models have been proposed that are able to predict each of the universal properties outlined above. However, no single model of text generation explains all of them together. Furthermore, no model has been used to interpret or predict the empirical distributions of text similarity between documents in a collection <xref ref-type="bibr" rid="pone.0005372-Menczer1">[23]</xref>, <xref ref-type="bibr" rid="pone.0005372-Menczer2">[24]</xref>.</p>
<p>In this paper, we present a model that generates collections of documents consistently with all of the above statistical features of textual corpora, and validate it against large and diverse Web datasets. We go beyond the global level of Zipf's law, which we take for granted, and focus on general correlation signatures within and across documents. These correlation patterns, manifesting themselves as burstiness and similarity, are destroyed when the words in a collection are reshuffled, even while the global word frequencies are preserved. Therefore the correlations are not simply explained by Zipf's law, and are directly related to the global organization and topicality of the corpora. The aim of our model is not to reproduce the microscopic patterns of occurrence of individual words, but rather to provide a stylized generative mechanism to interpret their emergence in statistical terms. Consequently, our main assumption is a global distribution of word probabilities; we do not need to fit a large number of parameters to the data, in contrast to parametric models proposed to describe the bursty nature or topicality of text <xref ref-type="bibr" rid="pone.0005372-Blei1">[25]</xref>–<xref ref-type="bibr" rid="pone.0005372-Elkan1">[27]</xref>. In our model, each document is derived by a local ranking of dynamically reordered words, and different documents are related by sharing subsets of these rankings that represent emerging topics. Our analysis shows that the statistical structure of text collections, including their level of topicality, can be derived from such a simple ranking mechanism. Ranking is an alternative to preferential attachment for explaining scale invariance <xref ref-type="bibr" rid="pone.0005372-Goh1">[28]</xref> and has been used to explain the emergent topology of complex information, technological, and social networks <xref ref-type="bibr" rid="pone.0005372-Fortunato1">[29]</xref>. The present results suggest that it may also shed light on cognitive processes such as text generation and the collective mechanisms we use to organize and store information.</p>
</sec><sec id="s2">
<title>Results and Discussion</title>
<sec id="s2a">
<title>Empirical Observations</title>
<p>We have selected three very diverse public datasets, from topically focused to broad coverage, to illustrate the statistical regularities of text and validate our model. The first corpus is the Industry Sector database (IS), a collection of corporate Web pages organized into categories or sectors. The second dataset is a sample of the Open Directory (ODP), a collection of Web pages classified into a large hierarchical taxonomy by volunteer editors. The third corpus is a random sample of topic pages from the English Wikipedia (Wiki), a popular collaborative encyclopaedia that also is comprised of millions of online entries. (See <xref ref-type="sec" rid="s3">Materials and Methods</xref> for details.)</p>
<p>We measured the statistical regularities mentioned above in our datasets and the empirical results are shown in <xref ref-type="fig" rid="pone-0005372-g001">Fig. 1</xref>. We stress that although our work focuses on collections of documents written in English, the regularities discussed here are universal and apply to documents written in virtually all languages. The distributions of document length for all three collections can be approximated by a lognormal with different first and second moment parameters <xref ref-type="bibr" rid="pone.0005372-Dolby1">[30]</xref> (see Web Datasets under <xref ref-type="sec" rid="s3">Materials and Methods</xref>). Another universal property of written text is Zipf's law <xref ref-type="bibr" rid="pone.0005372-Zipf1">[6]</xref>–<xref ref-type="bibr" rid="pone.0005372-Saichev1">[8]</xref>, <xref ref-type="bibr" rid="pone.0005372-Maslov1">[31]</xref>, according to which the global frequency <italic>f<sub>g</sub></italic> of terms in a collection decreases roughly inversely to their rank <italic>r</italic>: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e001" xlink:type="simple"/></inline-formula> or, in other words, the distribution of the frequency <italic>f<sub>g</sub></italic> is well approximated by a power law <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e002" xlink:type="simple"/></inline-formula> with exponent around <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e003" xlink:type="simple"/></inline-formula>. Zipf's law also applies to the datasets used here, as supported by a Kolmogorov-Smirnov goodness-of-fit test <xref ref-type="bibr" rid="pone.0005372-Clauset1">[32]</xref> (see <xref ref-type="fig" rid="pone-0005372-g001">Fig. 1a</xref> and its caption for details). Heaps' law <xref ref-type="bibr" rid="pone.0005372-Heaps1">[9]</xref>, <xref ref-type="bibr" rid="pone.0005372-Cattuto1">[10]</xref> describes the sublinear growth of vocabulary size (number of unique words) <italic>w</italic> as a function of the size of a document (number of words) <italic>n</italic> (<xref ref-type="fig" rid="pone-0005372-g001">Fig. 1b</xref>). This feature has also been observed in different languages, and the behavior has been interpreted as a power law <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e004" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e005" xlink:type="simple"/></inline-formula>, although the exponent <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e006" xlink:type="simple"/></inline-formula> between 0.4 and 0.6 is language dependent <xref ref-type="bibr" rid="pone.0005372-BaezaYates1">[33]</xref>.</p>
<fig id="pone-0005372-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0005372.g001</object-id><label>Figure 1</label><caption>
<title>Regularities in textual data as observed in our three empirical datasets.</title>
<p>(a) Zipf's Law: word counts are globally distributed according to a power law <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e007" xlink:type="simple"/></inline-formula>. The maximum likelihood estimates of the characteristic exponent <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e008" xlink:type="simple"/></inline-formula> are 1.83 for Wikipedia, 1.78 for IS, and 1.88 for ODP. A Kolmogorov-Smirnov goodness-of-fit test <xref ref-type="bibr" rid="pone.0005372-Clauset1">[32]</xref> comparing the original data against 2500 synthetic datasets gives p-values for the maximum likelihood fits of 1 for Wikipedia and IS and 0.56 for ODP, all well above a conservative threshold of 0.1. This ensures that the power-law distribution is a plausible and indeed very good model candidate for the real distributions. (b) Heaps' law: as the number of words <italic>n</italic> in a document grows, the average vocabulary size (i.e. the number of distinct words) <italic>w(n)</italic> grows sublinearly with <italic>n</italic>. (c) Burstiness: fraction of documents <italic>P(f<sub>d</sub>)</italic> containing <italic>f<sub>d</sub></italic> occurrences of common or rare terms. For each dataset, we label as “common” those terms that account for 71% of total word occurrences in the collection, while rare terms account for 8%. (d) Similarity: distribution of cosine similarity <italic>s</italic> across all pairs of documents, each represented as a term frequency vector. Also shown are <italic>w(n)</italic>, the distributions of <italic>f<sub>d</sub></italic>, and the distribution of <italic>s</italic> according to the Zipf null model (see text) corresponding to the IS dataset.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0005372.g001" xlink:type="simple"/></fig>
<p>Burstiness is the tendency of some words to occur clustered together in individual documents, so that a term is more likely to reappear in a document where it has appeared before <xref ref-type="bibr" rid="pone.0005372-Church1">[11]</xref>–<xref ref-type="bibr" rid="pone.0005372-Kleinberg1">[13]</xref>. This property is more evident among rare words, which are more likely to be topical. Following Elkan <xref ref-type="bibr" rid="pone.0005372-Elkan1">[27]</xref>, the bursty nature of words can be illustrated by dividing words into classes according to their global frequency (e.g., common vs. rare). For words in each class, we plot in <xref ref-type="fig" rid="pone-0005372-g001">Fig. 1c</xref> the probability <italic>P(f<sub>d</sub>)</italic> that these words occur with frequency <italic>f<sub>d</sub></italic> in single documents, averaged over all documents in the collection. We compare the distribution <italic>P(f<sub>d</sub>)</italic> of common and rare terms with those predicted by the null independence hypothesis. This reference model generates documents whose length is drawn from the lognormal distribution fitted to the empirical data (see <xref ref-type="sec" rid="s3">Materials and Methods</xref>) by drawing words independently at random from the global Zipf frequency distribution (<xref ref-type="fig" rid="pone-0005372-g001">Fig. 1a</xref>). As compared to the reference of such a <italic>Zipf model</italic>, rare terms are much more likely to cluster in specific documents and not to appear evenly distributed in the collection, so that ordering principles beyond those responsible for Zipf's law have to be at play.</p>
<p>Another signature of text collections, which is more telling about topicality, is the distribution of lexical similarity across pairs of documents. In information retrieval and text mining, documents are typically represented as term vectors <xref ref-type="bibr" rid="pone.0005372-Liu1">[15]</xref>, <xref ref-type="bibr" rid="pone.0005372-Salton1">[34]</xref>. Each element of a vector represents the weight of the corresponding term in the document. There are various vector representations according to different weighting schemes. Here, we focus on the simplest scheme, in which a weight is simply the frequency of the term in the document. The similarity between two documents is given by the cosine between the two vectors: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e009" xlink:type="simple"/></inline-formula> where <italic>w<sub>tp</sub></italic> is the weight of term <italic>t</italic> in document <italic>p</italic>. It has been observed that for documents sampled from the ODP, the distribution of cosine similarity based on term frequency vectors is concentrated around zero and decays in a roughly exponential fashion for <italic>s</italic>&gt;0 <xref ref-type="bibr" rid="pone.0005372-Menczer1">[23]</xref>, <xref ref-type="bibr" rid="pone.0005372-Menczer2">[24]</xref>. <xref ref-type="fig" rid="pone-0005372-g001">Figure 1d</xref> shows that different collections yield different similarity profiles, however they all tend to be more skewed toward small similarity values than predicted by the Zipf model.</p>
<p>Modeling how these properties emerge from simple rules is central to an understanding of human language and related cognitive processes. Our understanding, however, is far from definitive. First, the empirical observations are open to different interpretations. As an example, much has been written about the debate between Simon and Mandelbrot around different interpretations of Zipf's law (see <ext-link ext-link-type="uri" xlink:href="http://www.nslij-genetics.org/wli/zipf" xlink:type="simple">www.nslij-genetics.org/wli/zipf</ext-link> for a historical review of the debate). Second, and perhaps more importantly, no single model of text generation explains all of the above observations simultaneously. Third, models at hand are usually based on descriptive methods that cannot explain linguistic processes as emergent phenomena.</p>
<p>In the remainder of this paper, we focus on burstiness and similarity distributions. Regarding similarity, little attention has been given to its empirical distribution and, to the best of our knowledge, no model has been put forth to explain its profile. Regarding text burstiness, on the other hand, several models have been proposed including the two-Poisson model <xref ref-type="bibr" rid="pone.0005372-Church1">[11]</xref>, the Poisson zero-inflated mixture model <xref ref-type="bibr" rid="pone.0005372-Jansche1">[35]</xref>, Katz' k-mixture model <xref ref-type="bibr" rid="pone.0005372-Katz1">[12]</xref>, and a gap-based variation of Bayes model <xref ref-type="bibr" rid="pone.0005372-Sarkar1">[36]</xref>. Another line of generative models extends the simple multinomial family with increasingly complex views of topics. Examples include probabilistic latent semantic indexing <xref ref-type="bibr" rid="pone.0005372-Hofmann1">[37]</xref>, latent Dirichlet allocation (LDA) <xref ref-type="bibr" rid="pone.0005372-Blei1">[25]</xref>, and Pachinko allocation <xref ref-type="bibr" rid="pone.0005372-Li1">[38]</xref>. These models assume a set of topics, each typically described by a multinomial distribution over words. Each document is then generated from some mixture of these topics. In LDA, for example, the parameters of the mixture are drawn from a Dirichlet distribution, independently for each document. Repeatedly drawing a topic from the mixture first, and then drawing a term from the corresponding word distribution generate the words' sequence in a document. A variety of techniques have been developed to estimate from data the parameters that characterize the many distributions involved in the generative process <xref ref-type="bibr" rid="pone.0005372-Newman1">[21]</xref>, <xref ref-type="bibr" rid="pone.0005372-Griffiths1">[26]</xref>, <xref ref-type="bibr" rid="pone.0005372-Griffiths2">[39]</xref>. Although the above models were mainly developed for subject classification, they have also been used to investigate burstiness since bursty words can characterize the topic of a document <xref ref-type="bibr" rid="pone.0005372-Elkan1">[27]</xref>, <xref ref-type="bibr" rid="pone.0005372-Madsen1">[40]</xref>.</p>
<p>The very large numbers of free parameters associated with individual terms, topics, and/or their mixtures grant the above models great descriptive power. However, their cognitive plausibility is problematic. Our aim here is instead to produce a simpler, more plausible mechanism compatible with the high-level statistical regularities associated with <italic>both</italic> burstiness and similarity distributions, without regard for explicit topic modeling.</p>
</sec><sec id="s2b">
<title>Model and Validation</title>
<p>Two basic mechanisms, reordering and memory, can explain burstiness and similarity consistently with Zipf's law. We show this by proposing a generative model that incorporates these processes to produce collections of documents characterized by the observed statistical regularities. Each document is derived by a local ranking of words that reorganizes according to the changing word frequencies as the document grows, and different documents are related by sharing subsets of these rankings that represent emerging topics. With just the main assumptions of the global distribution of word probabilities and document sizes and a single tunable parameter measuring the topicality of the collection, we are able to generate synthetic corpora that re-create faithfully the features of our Web datasets. Next, we describe two variations of the model, one without memory and the second with a memory mechanism that captures topicality.</p>
<sec id="s2b1">
<title>Dynamic Ranking by Frequency</title>
<p>In our model, <italic>D</italic> documents are generated drawing word instances repeatedly with replacement from a vocabulary of <italic>V</italic> words. The document lengths in number of words are drawn from a lognormal distribution. The parameters <italic>D</italic>, <italic>V</italic>, and the maximum likelihood estimates of the lognormal mean and variance are derived empirically from each dataset (see <xref ref-type="table" rid="pone-0005372-t001">Table 1</xref> in <xref ref-type="sec" rid="s3">Materials and Methods</xref>). We further assume that at any step of the generation process, word probabilities follow a Zipf distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e010" xlink:type="simple"/></inline-formula> where <italic>r(t)</italic> is the rank of term <italic>t</italic>. (We also tested the model using the empirical distributions of document length and word frequency for each collection and the results are essentially the same.) However, rather than keeping a fixed ranking, we imagine that words are sorted dynamically during the generation of each document according to the number of times they have already occurred. Words and ranks are thus decoupled: at different times, a word can have different ranks and a position in the ranking can be occupied by different words. The idea is that as the topicality of a document emerges through its content, topical words will be more likely to reoccur within the same document. This idea is incorporated into the model as a frequency bias favoring words that occur early in the document.</p>
<table-wrap id="pone-0005372-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0005372.t001</object-id><label>Table 1</label><caption>
<title>Statistics for the different document collections.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pone-0005372-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0005372.t001" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">Dataset</td>
<td align="left" colspan="1" rowspan="1"><italic>V</italic></td>
<td align="left" colspan="1" rowspan="1"><italic>D</italic></td>
<td align="left" colspan="1" rowspan="1"><italic>&lt;w&gt;</italic></td>
<td align="left" colspan="1" rowspan="1"><italic>&lt;n&gt;</italic></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e011" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e012" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1"><inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e013" xlink:type="simple"/></inline-formula></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1"><bold>Wiki</bold></td>
<td align="left" colspan="1" rowspan="1">588639</td>
<td align="left" colspan="1" rowspan="1">100000 (0)</td>
<td align="left" colspan="1" rowspan="1">160.44</td>
<td align="left" colspan="1" rowspan="1">373.86</td>
<td align="left" colspan="1" rowspan="1">457083</td>
<td align="left" colspan="1" rowspan="1">5.13</td>
<td align="left" colspan="1" rowspan="1">1.57</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><bold>IS</bold></td>
<td align="left" colspan="1" rowspan="1">47979</td>
<td align="left" colspan="1" rowspan="1">9556 (15)</td>
<td align="left" colspan="1" rowspan="1">124.26</td>
<td align="left" colspan="1" rowspan="1">313.46</td>
<td align="left" colspan="1" rowspan="1">566409</td>
<td align="left" colspan="1" rowspan="1">4.81</td>
<td align="left" colspan="1" rowspan="1">2.10</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1"><bold>ODP</bold></td>
<td align="left" colspan="1" rowspan="1">105692</td>
<td align="left" colspan="1" rowspan="1">107360 (32558)</td>
<td align="left" colspan="1" rowspan="1">8.88</td>
<td align="left" colspan="1" rowspan="1">10.34</td>
<td align="left" colspan="1" rowspan="1">345</td>
<td align="left" colspan="1" rowspan="1">1.93</td>
<td align="left" colspan="1" rowspan="1">1.39</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt101"><label/><p><italic>V</italic> stands for vocabulary size, <italic>D</italic> for the number of documents containing at least one word (in parenthesis the number of empty documents in the collection), <italic>&lt;w&gt;</italic> for the average size of documents in number of unique words, and <italic>&lt;n&gt;</italic> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e014" xlink:type="simple"/></inline-formula> for the average and variance of document size in number of words. For each collection, the distribution of document size is approximately fitted by a lognormal with parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e015" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e016" xlink:type="simple"/></inline-formula> (values shown are maximum likelihood estimates).</p></fn></table-wrap-foot></table-wrap>
<p>In the first version of the model, each document is produced independently of each other. Before each new document is generated, words are sorted according to an initial global ranking, which remains fixed for all documents. This ranking <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e017" xlink:type="simple"/></inline-formula> is also used to break ties during the generation of documents, among words with the same occurrence counts. The algorithm corresponding to this dynamic ranking model is illustrated in <xref ref-type="fig" rid="pone-0005372-g002">Fig. 2</xref> and detailed in <xref ref-type="sec" rid="s3">Materials and Methods</xref>.</p>
<fig id="pone-0005372-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0005372.g002</object-id><label>Figure 2</label><caption>
<title>Illustration of the dynamic ranking model.</title>
<p>The parameter <italic>z</italic> regulates the lexical diversity, or topicality of the collection. The extreme case <italic>z = 0</italic> is equivalent to the null Zipf model, where all documents are generated using the global word rank distribution. The opposite case <italic>z = 1</italic> is the first version of the dynamic ranking model, with no memory, in which each new document starts from the global word ranking <italic>r<sub>0</sub></italic>. Intermediate values of <italic>z</italic> represent the more general version of the dynamic ranking model, where correlations across documents are created by a partial memory of word ranks. A more detailed algorithmic description of the model can be found in <xref ref-type="sec" rid="s3">Materials and Methods</xref>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0005372.g002" xlink:type="simple"/></fig>
<p>When a sufficiently large number of documents is generated, the measured frequency of a word <italic>t</italic> over the entire corpus approaches the Zipf distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e018" xlink:type="simple"/></inline-formula>, ensuring the self consistency of the model. We numerically simulated the dynamic ranking model for each dataset. A direct comparison with the empirical burstiness curves shown in <xref ref-type="fig" rid="pone-0005372-g001">Fig. 1c</xref> can be found in <xref ref-type="fig" rid="pone-0005372-g003">Fig. 3a</xref>. The excellent agreement suggests that the dynamic ranking process is sufficient for producing the right amount of correlations inside documents needed to realistically account for the burstiness effect.</p>
<fig id="pone-0005372-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0005372.g003</object-id><label>Figure 3</label><caption>
<title>Model vs. empirical observations.</title>
<p>The coefficient of determination R<sup>2</sup> is computed in all cases as an estimator of the goodness of fit between the simulation and the empirical measurements. (a) Comparison of burstiness curves produced by the dynamic ranking model with those from the empirical datasets. Common and rare words are defined in <xref ref-type="fig" rid="pone-0005372-g001">Fig. 1c</xref>. For all the comparisons, R<sup>2</sup> is larger than 0.99. (b) Comparison of Heaps' law curves produced by the dynamic ranking model with those from the empirical datasets. Simulations of the model provide the same predictions as numerical integration of the analytically derived equation using the empirical rank distributions (see text). For the IS dataset we also plot the result of the Zipf null model, which produces a sublinear <italic>w(n)</italic>, although less pronounced than our model. The ODP collection has short documents on average (cf. <xref ref-type="table" rid="pone-0005372-t001">Table 1</xref> in <xref ref-type="sec" rid="s3">Materials and Methods</xref>), so Heaps' law is barely observable. For all the comparisons, R<sup>2</sup> is larger than 0.99. (c) Comparison between similarity distributions produced by the dynamic ranking model with memory, and those from the empirical datasets also shown in <xref ref-type="fig" rid="pone-0005372-g001">Fig. 1d</xref>. The parameter <italic>z</italic> controlling the topical memory is fitted to the data. The peak at <italic>s</italic> = 0 suggests that the most common case is always that of documents sharing very few or no common terms. The discordance for high similarity values is due to corpus artifacts such as mirrored pages, templates, and very short (one word) documents. The fluctuations in the curves for the ODP dataset are due to binning artifacts for short pages. Also shown is the prediction of the topic model for the IS dataset (see text). Finally, the R<sup>2</sup> statistic has a value 0.98 for Wikipedia, 0.94 for IS, and larger than 0.99 for ODP.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0005372.g003" xlink:type="simple"/></fig>
<p>Heaps' law can be derived analytically from our model. The probability <italic>P(w,n)</italic> to find <italic>w</italic> distinct words in a document of size <italic>n</italic> satisfies the following discrete master equation:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0005372.e019" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e020" xlink:type="simple"/></inline-formula>, and <italic>P(r)</italic> is the Zipf probability associated with rank <italic>r</italic>.</p>
<p>There are two contributions to the probability to have <italic>w+1</italic> distinct words in a document of length <italic>n+1</italic>, represented by the two terms in the <italic>r.h.s</italic> of Eq. (1) above. Before adding the <italic>(n+1)</italic><sup>th</sup> the document may already contain <italic>w+1</italic> distinct words, and such number remains the same if an already observed word is added. Since the <italic>w+1</italic> words that have been already observed occupy the first <italic>w+1</italic> position in the rank, one of them is observed with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e021" xlink:type="simple"/></inline-formula>, therefore the first contribution ensues. The other possibility is that the document contains only <italic>w</italic> distinct words and that a previously unobserved word is added. For the same reasons presented above this happen with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e022" xlink:type="simple"/></inline-formula>, and this accounts for the second contribution. To make progresses it is useful to write an equation for the expected number of distinct words. This can be done by multiplying both sides of Eq. (1) by <italic>(w+1)</italic> and summing over <italic>w</italic>. This leads to:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0005372.e023" xlink:type="simple"/><label>(2)</label></disp-formula></p>
<p>To simplify notations we will use <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e024" xlink:type="simple"/></inline-formula> to indicate the expected value of a function <italic>f(w)</italic> at step n. Using the fact that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e025" xlink:type="simple"/></inline-formula>, and that the term in curly brackets on the <italic>r.h.s.</italic> of Eq. (2) is null, one finds:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0005372.e026" xlink:type="simple"/><label>(3)</label></disp-formula></p>
<p>To further simplify notations, we pose <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e027" xlink:type="simple"/></inline-formula>. To close Eq. (3) in terms of <italic>w(n)</italic> we neglect fluctuations and assume that the probability to observe <italic>w</italic> distinct words in a document of size <italic>n</italic> is strongly peaked around <italic>w(n)</italic>. Eq. (3) can then be rewritten as:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0005372.e028" xlink:type="simple"/><label>(4)</label></disp-formula></p>
<p>It is convenient to take the continuous limit, replacing finite differences by derivative, and sums by integrals. One finally obtains:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0005372.e029" xlink:type="simple"/><label>(5)</label></disp-formula></p>
<p>Eq. (5) can be integrated numerically using the actual <italic>P(r)</italic> from the data. Alternatively, Eq. (5) can be solved analytically for special cases. Assuming a Zipf's law with a tail of the form <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e030" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e031" xlink:type="simple"/></inline-formula>, the solution is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e032" xlink:type="simple"/></inline-formula> and we recover Heaps' sublinear growth with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e033" xlink:type="simple"/></inline-formula> for large <italic>n</italic>. According to the Yule-Simon model <xref ref-type="bibr" rid="pone.0005372-Simon1">[41]</xref>, which interprets Zipf's law through a preferential attachment process, the rank distribution should have a tail with exponent <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e034" xlink:type="simple"/></inline-formula>. This is confirmed empirically in many English collections; for example our ODP and Wikipedia datasets yield Zipfian tails with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e035" xlink:type="simple"/></inline-formula> between 3/2 and 2. Our model predicts that in these cases Heaps' growth should be well approximated by a power law with exponent <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e036" xlink:type="simple"/></inline-formula> between 1/2 and 2/3, closely matching those reported for the English language <xref ref-type="bibr" rid="pone.0005372-BaezaYates1">[33]</xref>. Simulations using the empirically derived <italic>P(r)</italic> for each dataset display growth trends for large <italic>n</italic> that are in good agreement with the empirical behavior (<xref ref-type="fig" rid="pone-0005372-g003">Fig. 3b</xref>).</p>
</sec><sec id="s2b2">
<title>Topicality and Similarity</title>
<p>The agreement between empirical data and simulations of the model with respect to the similarity distributions gets worse for those datasets that are more topically focused. A new mechanism is needed to account for topical correlations between documents.</p>
<p>The model in the previous section generates collections of independent text documents, with specific but uncorrelated topics captured by the bursty terms. For each new document, the rank of each word <italic>t</italic> is initialized to its original value <italic>r<sub>0</sub>(t)</italic> so that each document has no bias toward any particular topic. The resulting synthetic corpora display broad coverage. However, real corpora may cover more or less specific topics. The stronger the semantic relationship between documents, the higher the likelihood they share common words. Such collection topicality needs to be taken into account to accurately reproduce the distribution of text similarity between documents.</p>
<p>To incorporate topical correlations into our model, we introduce a memory effect connecting word frequencies across different documents. Generative models with memory have already been proposed to explain Heaps' law <xref ref-type="bibr" rid="pone.0005372-Cattuto1">[10]</xref>. In our algorithm (see <xref ref-type="fig" rid="pone-0005372-g002">Fig. 2</xref> and <xref ref-type="sec" rid="s3">Materials and Methods</xref>) we replace the initialization step so that a portion of the initial ranking of the terms in each document is inherited from the previously generated document. In particular, the counts of the <italic>r<sup>*</sup></italic> top-ranked words are preserved while all the others are reset to zero. The rank <italic>r<sup>*</sup></italic> is drawn from an exponential distribution <italic>P(r<sup>*</sup>) = z(1-z)<sup>r*</sup></italic>, where <italic>z</italic> is a probability parameter that models the lexical diversity of the collection and <italic>r<sup>*</sup></italic> has expected value <italic>1/z-1</italic>, which can be interpreted as the collection's shared topicality.</p>
<p>This variation of the model does not interfere with the reranking mechanism described in the previous section, so that the burstiness effect is preserved. The idea is to interpolate between two extreme cases. The case <italic>z</italic> = 0, in which counts are never reset, converges to the null Zipf model. All documents share the same general terms, modeling a collection of unspecific documents. Here we expect a high similarity in spite of the independence among documents, because the words in all documents are drawn from the identical Zipf distribution. The other extreme case, <italic>z</italic> = 1, reduces to the original model, where all the counts are always initialized to zero before starting a document. In this case, the bursty words are numerous but not the same across different documents, modeling a situation in which each document is very specific but there is no shared topic across documents. Intermediate cases 0&lt;<italic>z</italic>&lt;1 allow us to model correlations across documents not only due to the common general terms, but also to topical (bursty) terms.</p>
<p>We simulated the dynamic ranking model with memory under the same conditions corresponding to our datasets, but additionally fitting the parameter <italic>z</italic> to match the empirical similarity distributions. The comparisons are shown in <xref ref-type="fig" rid="pone-0005372-g003">Fig. 3c</xref>. The similarity distribution for the ODP is best reproduced for <italic>z</italic> = 1, in accordance to the fact that this collection is overwhelmingly composed of very specific documents spanning all topics. In such a situation, the original model accurately reproduces the high diversity among document topics and there is no need for memory. In contrast, Wikipedia topic pages use a homogenous vocabulary due to their strict encyclopaedic style and the social consensus mechanism driving the generation of content. This is reflected in the value <italic>z</italic> = 0.005, corresponding to an average of 1/<italic>z</italic> = 200 common words whose frequencies are correlated across successive pairs of documents. The industry sector dataset provides us with an intermediate case in which pages deal with more focused, but semantically related topics. The best fit of the similarity distribution is obtained for <italic>z</italic> = 0.1.</p>
<p>With the fitted values for the shared topicality parameter <italic>z</italic>, the agreement between model and empirical similarity data in <xref ref-type="fig" rid="pone-0005372-g003">Fig. 3c</xref> is excellent over a broad range of similarity values. To better illustrate the significance of this result, let us compare it with the prediction of a simple topic model. For this purpose we assume a priori knowledge of the set of topics to be used for generating the documents. The IS dataset lends itself to this analysis because the pages are classified into twelve disjoint industry sectors, which can naturally be interpreted as unmixed topics. For each topic <italic>c</italic>, we measured the frequency of each term <italic>t</italic> and used it as a probability <italic>p(t|c)</italic> in a multinomial distribution. We generated the documents for each topic using the actual empirical values for the number of documents in the topic and the number of words in each document. As shown in <xref ref-type="fig" rid="pone-0005372-g003">Fig. 3c</xref>, the resulting similarity distribution is better than that of the Zipf model (where we assume a single global distribution), however the prediction is not nearly as good as that of our model.</p>
<p>Our model only requires a single free parameter <italic>z</italic> plus the global (Zipfian) distribution of word probabilities, which determines the initial ranking. Conversely, for the topic model we must have —or fit— the frequency distribution <italic>p(t|c)</italic> over all terms for each topic, which implies an extraordinary increase in the number of free parameters since, apart from potential differences in the functional forms, each distribution would rank the terms in a different order.</p>
<p>Aside from complexity issues, the ability to recover similarities suggests that the dynamic ranking model, though not as well informed as the topic model on the distributions of the specific topics, better captures word correlations. Topics emerge as a consequence of the correlations between bursty terms across documents as determined by <italic>z</italic>, but it is not necessary to predefine the number of topics or their distributions.</p>
</sec></sec><sec id="s2c">
<title>Conclusion</title>
<p>Our results show that key regularities of written text beyond Zipf's law, namely burstiness, topicality and their interrelation, can be accounted for on the basis of two simple mechanisms, namely frequency ranking with dynamic reordering and memory across documents, and can be modeled with an essentially parameter-free algorithm. The rank based approach is in line with other recent models in which ranking has been used to explain the emergent topology of complex information, technological, and social networks <xref ref-type="bibr" rid="pone.0005372-Fortunato1">[29]</xref>. It is not the first time that a generative model for text has walked parallel paths with models of network growth. A remarkable example is the Yule-Simon model for text generation <xref ref-type="bibr" rid="pone.0005372-Simon1">[41]</xref> that was later rediscovered in the context of citation analysis <xref ref-type="bibr" rid="pone.0005372-deSollaPrice1">[42]</xref>, and has recently found broad popularity in the complex networks literature <xref ref-type="bibr" rid="pone.0005372-Albert1">[43]</xref>.</p>
<p>Our approach applies to datasets where the temporal sequence of documents is not important, but burstiness has also been studied in contexts where time is a critical component <xref ref-type="bibr" rid="pone.0005372-Kleinberg1">[13]</xref>, <xref ref-type="bibr" rid="pone.0005372-Barabsi1">[44]</xref>, and even in human languages evolution <xref ref-type="bibr" rid="pone.0005372-Atkinson1">[45]</xref>. Further investigations in relation to topicality could attempt to explicitly demonstrate the role of the topicality correlation parameter by looking at the hierarchical structure of content classifications. Subsets of increasingly specific topics of the whole collection could be extracted to study how the parameter <italic>z</italic> changes and how it is related to external categorizations. The proposed model can also be used to study the co-evolution of content and citation structure in the scientific literature, social media such as the Wikipedia, and the Web at large <xref ref-type="bibr" rid="pone.0005372-Cattuto1">[10]</xref>, <xref ref-type="bibr" rid="pone.0005372-Menczer1">[23]</xref>, <xref ref-type="bibr" rid="pone.0005372-Kleinberg2">[46]</xref>, <xref ref-type="bibr" rid="pone.0005372-AlvarezLacalle1">[47]</xref>.</p>
<p>From a broader perspective, it seems natural that models of text generation should be based on similar cognitive mechanisms as models of human text processing since text production is a translation of semantic concepts in the brain into external lexical representations. Indeed, our model's connection between frequency ranking and burstiness of words provides a way to relate two key mechanisms adopted in modeling how humans process the lexicon: rank frequency <xref ref-type="bibr" rid="pone.0005372-Murray1">[48]</xref> and context diversity <xref ref-type="bibr" rid="pone.0005372-Adelman1">[49]</xref>. The latter, measured by the number of documents that contain a word, is related to burstiness since, given a term's overall collection frequency, higher burstiness implies lower context diversity. While tracking frequencies is a significant cognitive burden, our model suggests that simply recognizing that a term occurs more often than another in the first few lines of a document would suffice for detecting bursty words from their ranking and consequently the topic of the text.</p>
<p>In summary, a picture of how language structure and topicality emerge in written text as complex phenomena can shed light into the collective cognitive processes we use to organize and store information, and find broad practical applications, for instance, in topic detection, literature analysis, and Web mining.</p>
</sec></sec><sec id="s3">
<title>Materials and Methods</title>
<sec id="s3a">
<title>Web Datasets</title>
<p>We use three different datasets. The Industry Sector database is a collection of almost 10,000 corporate Web pages organized into 12 categories or sectors. The second dataset is a sample of the Open Directory Project, a collection of Web pages classified into a large hierarchical taxonomy by volunteer editors (<italic>dmoz.org</italic>). While the full ODP includes millions of pages, our collection comprises of approximately 150,000 pages, sampled uniformly from all top-level categories. The third corpus is a random sample of 100,000 topic pages from the English Wikipedia, a popular collaborative encyclopedia that also is comprised of millions of online entries (<italic>en.wikipedia.org</italic>).</p>
<p>These English text collections are derived from public data and are publicly available (the IS dataset is available at <ext-link ext-link-type="uri" xlink:href="http://www.cs.umass.edu/~mccallum/code-data.html" xlink:type="simple">www.cs.umass.edu/̃mccallum/code-data.html</ext-link>, the ODP and Wikipedia corpora are available upon request); have been used in several previous studies, allowing a cross check of our results; and are large enough for our purposes without being computationally unmanageable. The datasets are however very diverse in a number of ways. The IS corpus is relatively small and topically focused, while ODP and Wikipedia are larger and have broader coverage, as reflected in their vocabulary sizes. IS documents represent corporate content, while many Web pages in the ODP collection are individually authored. Wikipedia topics are collaboratively edited and thus represent the consensus of a community.</p>
<p>The distributions of document length for all three collections can be approximated by lognormals shown in <xref ref-type="fig" rid="pone-0005372-g004">Fig. 4</xref>, with different first and second moment parameters. The values shown in <xref ref-type="table" rid="pone-0005372-t001">Table 1</xref> summarize the main statistical features of the three collections (lognormal parameters are the maximum likelihood estimates). Before our analysis, all documents in each collection have been parsed to extract the text (removing HTML markup) and syntactic variations of words have been conflated using standard stemming techniques <xref ref-type="bibr" rid="pone.0005372-Porter1">[50]</xref>.</p>
<fig id="pone-0005372-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0005372.g004</object-id><label>Figure 4</label><caption>
<title>Distributions of documents' length for all three collections.</title>
<p>Each distribution can be approximated by a lognormal, with different first and second moment parameters obtained by maximum likelihood (ML) (see <xref ref-type="table" rid="pone-0005372-t001">Table 1</xref>).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0005372.g004" xlink:type="simple"/></fig></sec><sec id="s3b">
<title>Algorithm</title>
<p>The following algorithm implements the dynamic ranking model:</p>
<p>Vocabulary: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e037" xlink:type="simple"/></inline-formula></p>
<p>Initial ranking: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e038" xlink:type="simple"/></inline-formula></p>
<p>Repeat until <italic>D</italic> documents are generated:</p>
<p> Initialize term counts to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e039" xlink:type="simple"/></inline-formula> (*)</p>
<p> Draw <italic>L</italic> from lognormal (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e040" xlink:type="simple"/></inline-formula>)</p>
<p> Repeat until <italic>L</italic> terms are generated:</p>
<p>  Sort terms to obtain new rank <italic>r(t)</italic> according to <italic>c(t)</italic></p>
<p>   (break ties by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e041" xlink:type="simple"/></inline-formula>)</p>
<p>  Select term <italic>t</italic> with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e042" xlink:type="simple"/></inline-formula></p>
<p>  Add t to current document</p>
<p>  <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0005372.e043" xlink:type="simple"/></disp-formula></p>
<p> End of document</p>
<p>End of collection</p>
<p>The document initialization step (line marked with an asterisk in above pseudocode) is altered in the more general, memory version of the model (see main text). In particular we set to zero the counts <italic>c(t)</italic> not of all terms, but only of terms <italic>t</italic> such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e044" xlink:type="simple"/></inline-formula>. The rank <italic>r<sup>*</sup></italic> is drawn from an exponential distribution <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e045" xlink:type="simple"/></inline-formula> with expected value <italic>1</italic>/<italic>z-1</italic>, as discussed in the main text. In simpler terms, the counts of the <italic>r<sup>*</sup></italic> top-ranked words are preserved while all the others are reset to zero.</p>
<p>Algorithmically, terms are sorted by counts so that the top-ranked term <italic>t (r(t)</italic> = 1) has the highest <italic>c(t)</italic>. We iterate over the ranks <italic>r</italic>, flipping a biased coin for each term. As long as the coin returns false (probability 1-<italic>z</italic>), we preserve <italic>c(t(r))</italic>. As soon as the coin returns true (probability <italic>z</italic>), say for the term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e046" xlink:type="simple"/></inline-formula>, we reset all the counts for this and the following terms: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0005372.e047" xlink:type="simple"/></inline-formula> <italic>c(t(r))</italic> = 0.</p>
<p>The special case <italic>z = 1</italic> reverts to the original, memory-less model; all counts are reset to zero and each document restarts from the global Zipfian ranking <italic>r<sub>0</sub></italic>. The special case <italic>z = 0</italic> is equivalent to the Zipf null model as the term counts are never reset and thus rapidly converge to the global Zipfian frequency distribution.</p>
</sec></sec></body>
<back>
<ack>
<p>We thank Charles Elkan, Rich Shiffrin, Michael Jones, Alessandro Vespignani, Dragomir Radev, Vittorio Loreto, Ciro Cattuto, and Marián Boguñá for useful discussions. Jacob Ratkiewicz provided assistance in gathering and analyzing the Wikipedia dataset.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0005372-Hauser1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hauser</surname><given-names>MD</given-names></name>
<name name-style="western"><surname>Chomsky</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Fitch</surname><given-names>WT</given-names></name>
</person-group>             <year>2002</year>             <article-title>The faculty of language: What is it, who has it, and how did it evolve?</article-title>             <source>Science</source>             <volume>298</volume>             <fpage>1569</fpage>             <lpage>1579</lpage>          </element-citation></ref>
<ref id="pone.0005372-Joshi1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Joshi</surname><given-names>AK</given-names></name>
</person-group>             <year>1991</year>             <article-title>Natural language processing.</article-title>             <source>Science</source>             <volume>253</volume>             <fpage>1242</fpage>             <lpage>1249</lpage>          </element-citation></ref>
<ref id="pone.0005372-Manning1"><label>3</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Manning</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Schütze</surname><given-names>H</given-names></name>
</person-group>             <year>1999</year>             <source>Foundations of statistical natural language processing</source>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>MIT press</publisher-name>          </element-citation></ref>
<ref id="pone.0005372-Nowak1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nowak</surname><given-names>MA</given-names></name>
<name name-style="western"><surname>Komarova</surname><given-names>NL</given-names></name>
<name name-style="western"><surname>Niyogi</surname><given-names>P</given-names></name>
</person-group>             <year>2002</year>             <article-title>Computational and evolutionary aspects of Language.</article-title>             <source>Nature</source>             <volume>417</volume>             <fpage>611</fpage>             <lpage>617</lpage>          </element-citation></ref>
<ref id="pone.0005372-Chomsky1"><label>5</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Chomsky</surname><given-names>N</given-names></name>
</person-group>             <year>2006</year>             <source>Language and Mind</source>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>Cambridge University Press, 3rd ed</publisher-name>          </element-citation></ref>
<ref id="pone.0005372-Zipf1"><label>6</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Zipf</surname><given-names>GK</given-names></name>
</person-group>             <year>1949</year>             <source>Human Behaviour and the Principle of Least Effort</source>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>Addison-Wesley</publisher-name>          </element-citation></ref>
<ref id="pone.0005372-Baayen1"><label>7</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Baayen</surname><given-names>RH</given-names></name>
</person-group>             <year>2001</year>             <source>Word Frequency Distributions</source>             <publisher-loc>Dordrecht, Boston, London</publisher-loc>             <publisher-name>Kluwer Academic Publishers</publisher-name>          </element-citation></ref>
<ref id="pone.0005372-Saichev1"><label>8</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Saichev</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Malevergne</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Sornette</surname><given-names>D</given-names></name>
</person-group>             <year>2008</year>             <source>Theory of Zipf's Law and of General Power Law Distributions with Gibrat's Law of Proportional Growth, Lecture Notes in Economics and Mathematical Systems</source>             <publisher-loc>Berlin, Heidelberg, New York</publisher-loc>             <publisher-name>Springer</publisher-name>          </element-citation></ref>
<ref id="pone.0005372-Heaps1"><label>9</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Heaps</surname><given-names>HS</given-names></name>
</person-group>             <year>1978</year>             <source>Information Retrieval: Computational and Theoretical Aspects</source>             <publisher-loc>Orlando</publisher-loc>             <publisher-name>Academic Press</publisher-name>          </element-citation></ref>
<ref id="pone.0005372-Cattuto1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Cattuto</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Loreto</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Pietronero</surname><given-names>L</given-names></name>
</person-group>             <year>2007</year>             <article-title>Semiotic dynamics and collaborative tagging.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>104</volume>             <fpage>1461</fpage>             <lpage>1464</lpage>          </element-citation></ref>
<ref id="pone.0005372-Church1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Church</surname><given-names>KW</given-names></name>
<name name-style="western"><surname>Gale</surname><given-names>WA</given-names></name>
</person-group>             <year>1995</year>             <article-title>Poisson mixtures.</article-title>             <source>Natural Language Engineering</source>             <fpage>163</fpage>             <lpage>190</lpage>          </element-citation></ref>
<ref id="pone.0005372-Katz1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Katz</surname><given-names>SM</given-names></name>
</person-group>             <year>1996</year>             <article-title>Distribution of content words and phrases in text and language modelling.</article-title>             <source>Natural Language Engineering</source>             <volume>2</volume>             <fpage>15</fpage>             <lpage>59</lpage>          </element-citation></ref>
<ref id="pone.0005372-Kleinberg1"><label>13</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kleinberg</surname><given-names>J</given-names></name>
</person-group>             <year>2002</year>             <article-title>Bursty and hierarchical structure in streams.</article-title>             <comment>In: Proc 8th ACM SIGKDD Intl Conf on Knowledge Discovery and Data Mining</comment>          </element-citation></ref>
<ref id="pone.0005372-Chakrabarti1"><label>14</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Chakrabarti</surname><given-names>S</given-names></name>
</person-group>             <year>2003</year>             <source>Mining the Web: Discovering knowledge from hypertext data</source>             <publisher-loc>San Francisco</publisher-loc>             <publisher-name>Morgan Kaufmann</publisher-name>          </element-citation></ref>
<ref id="pone.0005372-Liu1"><label>15</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Liu</surname><given-names>B</given-names></name>
</person-group>             <year>2007</year>             <source>Web Data Mining: Exploring Hyperlinks, Contents and Usage Data</source>             <publisher-loc>Berlin, Heidelberg, New York</publisher-loc>             <publisher-name>Springer</publisher-name>          </element-citation></ref>
<ref id="pone.0005372-Ananiadou1"><label>16</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="editor">
<name name-style="western"><surname>Ananiadou</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Mcnaught</surname><given-names>J</given-names></name>
</person-group>             <year>2005</year>             <source>Text Mining for Biology And Biomedicine</source>             <publisher-loc>Norwood</publisher-loc>             <publisher-name>Artech House Publishers</publisher-name>          </element-citation></ref>
<ref id="pone.0005372-Feldman1"><label>17</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Feldman</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Sanger</surname><given-names>J</given-names></name>
</person-group>             <year>2006</year>             <source>The Text Mining Handbook: Advanced Approaches in Analyzing Unstructured Data</source>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>Cambridge University Press</publisher-name>          </element-citation></ref>
<ref id="pone.0005372-Allan1"><label>18</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Allan</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Papka</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Lavrenko</surname><given-names>V</given-names></name>
</person-group>             <year>1998</year>             <article-title>On-line new event detection and tracking.</article-title>             <fpage>37</fpage>             <lpage>45</lpage>             <comment>In: Proc SIGIR</comment>          </element-citation></ref>
<ref id="pone.0005372-Yang1"><label>19</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Yang</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Pierce</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Carbonell</surname><given-names>JG</given-names></name>
</person-group>             <year>1998</year>             <article-title>A study of retrospective and on-line event detection.</article-title>             <fpage>28</fpage>             <lpage>36</lpage>             <comment>In: Proc SIGIR</comment>          </element-citation></ref>
<ref id="pone.0005372-Chen1"><label>20</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Chen</surname><given-names>H</given-names></name>
</person-group>             <year>2006</year>             <source>Intelligence and Security Informatics for International Security Information Sharing and Data Mining</source>             <publisher-loc>Berlin, Heidelberg, New York</publisher-loc>             <publisher-name>Springer</publisher-name>          </element-citation></ref>
<ref id="pone.0005372-Newman1"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Newman</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Chemudugunta</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Smyth</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Steyvers</surname><given-names>M</given-names></name>
</person-group>             <year>2006</year>             <article-title>Analyzing entities and topics in news articles using statistical topic models.</article-title>             <source>Lecture Notes in Computer Science (Intelligence and Security Informatics)</source>             <volume>3975</volume>             <fpage>93</fpage>             <lpage>104</lpage>          </element-citation></ref>
<ref id="pone.0005372-Pennebaker1"><label>22</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pennebaker</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Chung</surname><given-names>C</given-names></name>
</person-group>             <year>2008</year>             <article-title>Computerized text analysis of al-qaeda transcripts.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Krippendor</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Bock</surname><given-names>M</given-names></name>
</person-group>             <source>A content analysis reader</source>             <publisher-loc>Thousand Oaks</publisher-loc>             <publisher-name>Sage</publisher-name>          </element-citation></ref>
<ref id="pone.0005372-Menczer1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Menczer</surname><given-names>F</given-names></name>
</person-group>             <year>2004</year>             <article-title>The evolution of document networks.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>101</volume>             <fpage>5261</fpage>             <lpage>5265</lpage>          </element-citation></ref>
<ref id="pone.0005372-Menczer2"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Menczer</surname><given-names>F</given-names></name>
</person-group>             <year>2002</year>             <article-title>Growing and navigating the small world Web by local content .</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>99</volume>             <fpage>14014</fpage>             <lpage>14019</lpage>          </element-citation></ref>
<ref id="pone.0005372-Blei1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Blei</surname><given-names>DM</given-names></name>
<name name-style="western"><surname>Ng</surname><given-names>AY</given-names></name>
<name name-style="western"><surname>Jordan</surname><given-names>MI</given-names></name>
</person-group>             <year>2003</year>             <article-title>Latent dirichlet allocation.</article-title>             <source>Journal of Machine Learning Research</source>             <volume>3</volume>             <fpage>993</fpage>             <lpage>1022</lpage>          </element-citation></ref>
<ref id="pone.0005372-Griffiths1"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Griffiths</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Steyvers</surname></name>
</person-group>             <year>2004</year>             <article-title>Finding scientific topics.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <fpage>5228</fpage>             <lpage>5235</lpage>          </element-citation></ref>
<ref id="pone.0005372-Elkan1"><label>27</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Elkan</surname><given-names>C</given-names></name>
</person-group>             <year>2006</year>             <article-title>Clustering documents with an exponential-family approximation of the dirichlet compound multinomial distribution.</article-title>             <comment>In: Proc 23rd Intl Conf on Machine Learning (ICML)</comment>          </element-citation></ref>
<ref id="pone.0005372-Goh1"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Goh</surname><given-names>K-I</given-names></name>
<name name-style="western"><surname>Kahng</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Kim</surname><given-names>D</given-names></name>
</person-group>             <year>2001</year>             <article-title>Universal behavior of load distribution in scale-free networks.</article-title>             <source>Phys Rev Lett</source>             <volume>87</volume>             <fpage>278701</fpage>          </element-citation></ref>
<ref id="pone.0005372-Fortunato1"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fortunato</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Flammini</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Menczer</surname><given-names>F</given-names></name>
</person-group>             <year>2006</year>             <article-title>Scale-free network growth by ranking.</article-title>             <source>Phys Rev Lett</source>             <volume>96</volume>             <fpage>218701</fpage>          </element-citation></ref>
<ref id="pone.0005372-Dolby1"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dolby</surname><given-names>JL</given-names></name>
</person-group>             <year>1971</year>             <article-title>Programming languages in mechanized documentation.</article-title>             <source>Journal of Documentation</source>             <volume>27</volume>             <fpage>136</fpage>             <lpage>155</lpage>          </element-citation></ref>
<ref id="pone.0005372-Maslov1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Maslov</surname><given-names>VP</given-names></name>
<name name-style="western"><surname>Maslova</surname><given-names>TV</given-names></name>
</person-group>             <year>2006</year>             <article-title>On zipf's law and rank distributions in linguistics and semiotics.</article-title>             <source>Mathematical Notes</source>             <volume>80</volume>             <fpage>679</fpage>             <lpage>691</lpage>          </element-citation></ref>
<ref id="pone.0005372-Clauset1"><label>32</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Clauset</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Shalizi</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Newman</surname><given-names>MEJ</given-names></name>
</person-group>             <year>2009</year>             <article-title>Power-law distributions in empirical data.</article-title>             <comment>SIAM Review, to appear</comment>          </element-citation></ref>
<ref id="pone.0005372-BaezaYates1"><label>33</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Baeza-Yates</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Ribeiro-Neto</surname><given-names>B</given-names></name>
</person-group>             <year>1999</year>             <source>Modern Information Retrieval</source>             <publisher-loc>Wokingham</publisher-loc>             <publisher-name>Addison-Wesley</publisher-name>          </element-citation></ref>
<ref id="pone.0005372-Salton1"><label>34</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Salton</surname><given-names>G</given-names></name>
<name name-style="western"><surname>McGill</surname><given-names>M</given-names></name>
</person-group>             <year>1983</year>             <source>An Introduction to Modern Information Retrieval</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>McGraw-Hill</publisher-name>          </element-citation></ref>
<ref id="pone.0005372-Jansche1"><label>35</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Jansche</surname><given-names>M</given-names></name>
</person-group>             <year>2003</year>             <article-title>Parametric models of linguistic count data.</article-title>             <fpage>288</fpage>             <lpage>295</lpage>             <comment>In: Proc 41st Annual Meeting of the Association for Computational Linguistics</comment>          </element-citation></ref>
<ref id="pone.0005372-Sarkar1"><label>36</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sarkar</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Garthwaite</surname><given-names>P</given-names></name>
<name name-style="western"><surname>De Roeck</surname><given-names>A</given-names></name>
</person-group>             <year>2005</year>             <article-title>A Bayesian mixture model for term reoccurrence and burstiness.</article-title>             <fpage>48</fpage>             <lpage>55</lpage>             <comment>In: Proc 9th Conference on Computational Natural Language Learning</comment>          </element-citation></ref>
<ref id="pone.0005372-Hofmann1"><label>37</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hofmann</surname><given-names>T</given-names></name>
</person-group>             <year>1999</year>             <article-title>Probabilistic latent semantic indexing.</article-title>             <fpage>50</fpage>             <lpage>57</lpage>             <comment>In: Proc 22th Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval</comment>          </element-citation></ref>
<ref id="pone.0005372-Li1"><label>38</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Li</surname><given-names>W</given-names></name>
<name name-style="western"><surname>McCallum</surname><given-names>A</given-names></name>
</person-group>             <year>2006</year>             <article-title>Pachinko allocation: DAG-structured mixture models of topic correlations.</article-title>             <fpage>577</fpage>             <lpage>584</lpage>             <comment>In: Proc 23rd Intl Conf on Machine Learning (ICML)</comment>          </element-citation></ref>
<ref id="pone.0005372-Griffiths2"><label>39</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Griffiths</surname><given-names>TL</given-names></name>
<name name-style="western"><surname>Steyvers</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Blei</surname><given-names>DM</given-names></name>
<name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name>
</person-group>             <year>2005</year>             <article-title>Integrating topics and Syntax.</article-title>             <source>Advances in Neural Information Processing Systems</source>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>MIT Press. vol. 17</publisher-name>             <fpage>537</fpage>             <lpage>544</lpage>          </element-citation></ref>
<ref id="pone.0005372-Madsen1"><label>40</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Madsen</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Kauchak</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Elkan</surname><given-names>C</given-names></name>
</person-group>             <year>2005</year>             <article-title>Modeling word burstiness using the dirichlet distribution.</article-title>             <fpage>545</fpage>             <lpage>552</lpage>             <comment>In Proc 22nd Intl Conf on Machine Learning (ICML)</comment>          </element-citation></ref>
<ref id="pone.0005372-Simon1"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Simon</surname><given-names>HA</given-names></name>
</person-group>             <year>1955</year>             <article-title>On a class of skew distribution functions.</article-title>             <source>Biometrika</source>             <volume>42</volume>             <fpage>425</fpage>             <lpage>440</lpage>          </element-citation></ref>
<ref id="pone.0005372-deSollaPrice1"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>de Solla Price</surname><given-names>D</given-names></name>
</person-group>             <year>1976</year>             <article-title>A general theory of bibliometric and other cumulative advantage processes.</article-title>             <source>J Amer Soc Inform Sci</source>             <volume>27</volume>             <fpage>292</fpage>             <lpage>306</lpage>          </element-citation></ref>
<ref id="pone.0005372-Albert1"><label>43</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Albert</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Barabási</surname><given-names>A-L</given-names></name>
</person-group>             <year>2002</year>             <article-title>Statistical mechanics of complex networks.</article-title>             <source>Reviews of Modern Physics</source>             <volume>74</volume>             <fpage>47</fpage>             <lpage>97</lpage>          </element-citation></ref>
<ref id="pone.0005372-Barabsi1"><label>44</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Barabási</surname><given-names>A-L</given-names></name>
</person-group>             <year>2005</year>             <article-title>The origin of bursts and heavy tails in human dynamics.</article-title>             <source>Nature</source>             <volume>435</volume>             <fpage>207</fpage>             <lpage>211</lpage>          </element-citation></ref>
<ref id="pone.0005372-Atkinson1"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Atkinson</surname><given-names>QD</given-names></name>
<name name-style="western"><surname>Meade</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Venditti</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Greenhill</surname><given-names>SJ</given-names></name>
<name name-style="western"><surname>Pagel</surname><given-names>M</given-names></name>
</person-group>             <year>2008</year>             <article-title>Languages evolve in punctuational bursts.</article-title>             <source>Science</source>             <volume>319</volume>             <fpage>588</fpage>          </element-citation></ref>
<ref id="pone.0005372-Kleinberg2"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Kleinberg</surname><given-names>J</given-names></name>
</person-group>             <year>2004</year>             <article-title>Analysing the scientific literature in its online context.</article-title>             <source>Nature Web Focus on Access to the Literature</source>          </element-citation></ref>
<ref id="pone.0005372-AlvarezLacalle1"><label>47</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Alvarez-Lacalle</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Dorow</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Eckmann</surname><given-names>J-P</given-names></name>
<name name-style="western"><surname>Moses</surname><given-names>E</given-names></name>
</person-group>             <year>2006</year>             <article-title>Hierarchical structures induce long-range dynamical correlations in written texts.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>103</volume>             <fpage>7956</fpage>             <lpage>7961</lpage>          </element-citation></ref>
<ref id="pone.0005372-Murray1"><label>48</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Murray</surname><given-names>WS</given-names></name>
<name name-style="western"><surname>Forster</surname><given-names>KI</given-names></name>
</person-group>             <year>2004</year>             <article-title>Seriel mechanisms in lexical access:</article-title>             <source>The Rank Hypothesis Psychological Review</source>             <volume>111</volume>             <fpage>721</fpage>             <lpage>756</lpage>          </element-citation></ref>
<ref id="pone.0005372-Adelman1"><label>49</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Adelman</surname><given-names>JS</given-names></name>
<name name-style="western"><surname>Brown</surname><given-names>GDA</given-names></name>
<name name-style="western"><surname>Quesada</surname><given-names>JF</given-names></name>
</person-group>             <year>2006</year>             <article-title>Contextual diversity, not word frequency, determines word naming and lexical decision times.</article-title>             <source>Psychological Science</source>             <volume>17</volume>             <fpage>814</fpage>             <lpage>823</lpage>          </element-citation></ref>
<ref id="pone.0005372-Porter1"><label>50</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Porter</surname><given-names>M</given-names></name>
</person-group>             <year>1980</year>             <article-title>An algorithm for suffix stripping.</article-title>             <source>Program</source>             <volume>14</volume>             <fpage>130</fpage>             <lpage>137</lpage>          </element-citation></ref>
</ref-list>

</back>
</article>